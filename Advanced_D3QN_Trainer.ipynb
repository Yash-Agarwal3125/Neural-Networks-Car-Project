{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82df08ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash\\OneDrive\\Desktop\\Neural Networks Car Project\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model, layers, optimizers\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pygame\n",
    "import numpy as np\n",
    "from Core_Game_Parts import *\n",
    "import os \n",
    "import pandas as pda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef28c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dueling_dqn(input_shape, action_size):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Value stream\n",
    "    value = layers.Dense(64, activation='relu')(x)\n",
    "    value = layers.Dense(1, activation='linear')(value)\n",
    "\n",
    "    # Advantage stream\n",
    "    advantage = layers.Dense(64, activation='relu')(x)\n",
    "    advantage = layers.Dense(action_size, activation='linear')(advantage)\n",
    "\n",
    "    # Combine value and advantage\n",
    "    q_values = layers.Lambda(lambda a: a[0] + (a[1] - tf.reduce_mean(a[1], axis=1, keepdims=True)))([value, advantage])\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=q_values)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=2e-4, clipnorm=1.0), loss='mse')\n",
    "\n",
    "    return model\n",
    "class PERMemory:\n",
    "    def __init__(self, capacity, alpha=0.5):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def add(self, experience, td_error):\n",
    "        priority = (abs(td_error) + self.epsilon) ** self.alpha\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities / np.sum(priorities)\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return experiences, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for i, td_error in zip(indices, td_errors):\n",
    "            self.priorities[i] = (abs(td_error) + self.epsilon) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "class D3QNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.98\n",
    "        self.batch_size = 128\n",
    "        self.tau = 0.005\n",
    "        self.memory = PERMemory(20000)\n",
    "\n",
    "        self.model = build_dueling_dqn((state_size,), action_size)\n",
    "        self.target_model = build_dueling_dqn((state_size,), action_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        q_values = self.model.predict(np.expand_dims(state, 0), verbose=0)[0]\n",
    "        target_q = self.target_model.predict(np.expand_dims(next_state, 0), verbose=0)[0]\n",
    "        best_next_action = np.argmax(self.model.predict(np.expand_dims(next_state, 0), verbose=0)[0])\n",
    "        target = reward + self.gamma * target_q[best_next_action] * (1 - int(done))\n",
    "        td_error = target - q_values[action]\n",
    "        self.memory.add((state, action, reward, next_state, done), td_error)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch, indices, weights = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        next_qs = self.model.predict(next_states, verbose=0)\n",
    "        next_q_targets = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        td_errors = []\n",
    "        for i in range(self.batch_size):\n",
    "            best_action = np.argmax(next_qs[i])\n",
    "            target_value = rewards[i] + self.gamma * next_q_targets[i][best_action] * (1 - dones[i])\n",
    "            td_error = target_value - targets[i][actions[i]]\n",
    "            td_errors.append(td_error)\n",
    "            targets[i][actions[i]] += 0.1 * td_error\n",
    "\n",
    "        self.model.fit(states, targets, sample_weight=weights, epochs=1, verbose=0)\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        new_weights = []\n",
    "        for w, target_w in zip(self.model.get_weights(), self.target_model.get_weights()):\n",
    "            new_weights.append(self.tau * w + (1 - self.tau) * target_w)\n",
    "        self.target_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3c2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, render_mode=False):\n",
    "        # Disable rendering (headless)\n",
    "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "        pygame.init()\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "        screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        self.track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "        self.car = Car(CAR_IMAGE_PATH, 900, 426, angle=-45)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        self.state_size = 5   # 3 sensors + speed + curvature\n",
    "        self.action_size = 4  # left, right, brake, accelerate\n",
    "        self.max_steps = 2000\n",
    "        self.checkpoints_cleared = 0\n",
    "        self.prev_dist_to_next_checkpoint = None\n",
    "        self.current_checkpoint_idx = 0\n",
    "\n",
    "        self.current_checkpoint_idx = 0\n",
    "        self.checkpoints_cleared = 0\n",
    "        self.inside_checkpoint = False  \n",
    "        self.no_progress_steps = 0\n",
    "        self.curriculum_max_checkpoint = 1   # start with only checkpoint 0 → 1\n",
    "        self.total_checkpoints = len(checkpoint_data)\n",
    "\n",
    "        if self.render_mode:\n",
    "            self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        else:\n",
    "            self.screen = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_checkpoint_idx = 0\n",
    "        self.checkpoints_cleared = 0\n",
    "        self.inside_checkpoint = False\n",
    "        self.prev_dist_to_next_checkpoint = None\n",
    "\n",
    "        self.car = Car(CAR_IMAGE_PATH, 900, 426, angle=-45)\n",
    "        self.steps = 0\n",
    "        self.checkpoints_cleared = 0\n",
    "        self.prev_dist_to_next_checkpoint = None\n",
    "        self.current_checkpoint_idx = 0\n",
    "        return self._get_state()\n",
    "   \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "\n",
    "    # =====================================================\n",
    "    # 1. PHYSICS UPDATE (ACCEL / BRAKE / TURN)\n",
    "    # =====================================================\n",
    "        MAX_SPEED = 10\n",
    "        SAFE_TURN_SPEED = 3.5 \n",
    "        MIN_SPEED = 1.5\n",
    "\n",
    "    # Steering\n",
    "        if action == 0:      # left\n",
    "            self.car.angle += 5\n",
    "        elif action == 2:    # right\n",
    "            self.car.angle -= 5\n",
    "\n",
    "    # Speed control\n",
    "        if action == 1:      # accelerate\n",
    "            self.car.speed = min(self.car.speed + 0.15, MAX_SPEED)\n",
    "        elif action == 3:    # brake\n",
    "            self.car.speed = max(self.car.speed - 0.30, MIN_SPEED)\n",
    "        else:                # turning friction\n",
    "            self.car.speed = max(self.car.speed - 0.12, 2.0)\n",
    "        self.car.move()\n",
    "        self.steps += 1\n",
    "\n",
    "    # =====================================================\n",
    "    # 2. SENSOR READINGS\n",
    "    # =====================================================\n",
    "        sensor_distance, _ = ray_casting(self.car, self.track_surface)\n",
    "        left, front, right = sensor_distance\n",
    "        left/=200\n",
    "        front/=200\n",
    "        right/=200\n",
    "        curvature = abs(left - right) / max(left + right, 1.0)\n",
    "        curvature = np.clip(curvature, 0.0, 1.0)\n",
    "\n",
    "    # =====================================================\n",
    "    # 3. PROGRESS REWARD (MAIN OBJECTIVE)\n",
    "    # =====================================================\n",
    "        target_rect = checkpoint_data[self.current_checkpoint_idx]\n",
    "        target_x = target_rect[0] + target_rect[2] / 2\n",
    "        target_y = target_rect[1] + target_rect[3] / 2\n",
    "        curr_dist = np.hypot(self.car.x - target_x, self.car.y - target_y)\n",
    "\n",
    "        if self.prev_dist_to_next_checkpoint is None:\n",
    "            progress = 0.0\n",
    "        else:\n",
    "            progress = self.prev_dist_to_next_checkpoint - curr_dist\n",
    "\n",
    "        self.prev_dist_to_next_checkpoint = curr_dist\n",
    "        r_progress = np.clip(progress, -1.0, 1.0)\n",
    "\n",
    "        if r_progress < 0.2:\n",
    "            self.no_progress_steps += 1\n",
    "        else:\n",
    "            self.no_progress_steps = 0\n",
    "\n",
    "        if self.no_progress_steps > 250:\n",
    "            reward -= 5.0\n",
    "            done = True\n",
    "    # =====================================================\n",
    "    # 4. TURN-AWARE SPEED REWARD (KEY FIX)\n",
    "    # =====================================================\n",
    "\n",
    "        # Desired speed depends on curvature\n",
    "        # Straight → fast, Turn → slow\n",
    "        desired_speed = (\n",
    "            (1.0 - curvature) * MAX_SPEED + curvature * SAFE_TURN_SPEED\n",
    "            )\n",
    "        desired_speed = np.clip(desired_speed, MIN_SPEED, MAX_SPEED)\n",
    "        speed_error = self.car.speed - desired_speed\n",
    "\n",
    "        # Reward matching desired speed\n",
    "        r_speed = np.exp(-0.5 * (speed_error ** 2))\n",
    "        if curvature > 0.4 and action != 3:\n",
    "            reward -= 0.15\n",
    "\n",
    "        if curvature < 0.1 and self.car.speed < 6.0:\n",
    "            reward -= 0.01\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================\n",
    "    # 5. CENTERING REWARD (STABILITY)\n",
    "    # =====================================================\n",
    "        max_sensor = 200.0\n",
    "        r_center = (max_sensor - abs(front - 100.0)) / max_sensor\n",
    "\n",
    "    # =====================================================\n",
    "    # 6. ANTI-BAD BEHAVIOR PENALTIES\n",
    "    # =====================================================\n",
    "    # Penalize fast turning\n",
    "        if action in [0, 2] and self.car.speed > 3.5:\n",
    "            reward -= 1.5\n",
    "\n",
    "    # Small step penalty (encourage efficiency)\n",
    "        reward -= 0.05\n",
    "\n",
    "        # Enforce curriculum boundary\n",
    "        if self.current_checkpoint_idx >= self.curriculum_max_checkpoint and r_progress > 0:\n",
    "            # Do NOT allow further progress\n",
    "            reward -= 0.2\n",
    "\n",
    "    # =====================================================\n",
    "        # CURRICULUM-AWARE CHECKPOINT HANDLING\n",
    "        # =====================================================\n",
    "        checkpoint_hit = False\n",
    "\n",
    "        cp = checkpoint_data[self.current_checkpoint_idx]\n",
    "        cp_rect = pygame.Rect(cp[0], cp[1], cp[2], cp[3])\n",
    "        cp_rect.inflate_ip(40, 40)\n",
    "\n",
    "        car_rect = self.car.get_rect()\n",
    "        cp_center = (cp[0] + cp[2] / 2, cp[1] + cp[3] / 2)\n",
    "        dist_to_cp = np.hypot(self.car.x - cp_center[0], self.car.y - cp_center[1])\n",
    "\n",
    "        inside_now = car_rect.colliderect(cp_rect) or dist_to_cp < 70\n",
    "\n",
    "        # Edge-triggered\n",
    "        if inside_now and not self.inside_checkpoint:\n",
    "            checkpoint_hit = True\n",
    "            self.inside_checkpoint = True\n",
    "\n",
    "        if not inside_now:\n",
    "            self.inside_checkpoint = False\n",
    "        \n",
    "        # ----- Apply curriculum -----\n",
    "        if checkpoint_hit:\n",
    "            if self.current_checkpoint_idx < self.curriculum_max_checkpoint:\n",
    "                reward += 50.0\n",
    "                self.checkpoints_cleared += 1\n",
    "                self.current_checkpoint_idx += 1\n",
    "                self.prev_dist_to_next_checkpoint = None\n",
    "                if self.current_checkpoint_idx == 0 and self.checkpoints_cleared > 0:\n",
    "                    reward += 150.0\n",
    "            else:\n",
    "                # Hit a locked checkpoint\n",
    "                reward -= 2.0\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================\n",
    "    # 8. COLLISION CHECK\n",
    "    # =====================================================\n",
    "        x, y = int(self.car.x), int(self.car.y)\n",
    "        if x < 0 or y < 0 or x >= SCREEN_WIDTH or y >= SCREEN_HEIGHT:\n",
    "            reward = -10.0\n",
    "            done = True\n",
    "        else:\n",
    "            pixel = self.track_surface.get_at((x, y))[:3]\n",
    "            if pixel == DRAW_COLOR:\n",
    "                reward = -10.0\n",
    "                done = True\n",
    "\n",
    "    # =====================================================\n",
    "    # 9. FINAL REWARD COMPOSITION\n",
    "    # =====================================================\n",
    "        reward += (\n",
    "            2.0 * r_progress +\n",
    "            2.0 * r_speed +\n",
    "            0.3 * r_center\n",
    "        )\n",
    "\n",
    "    # =====================================================\n",
    "    # 10. TERMINATION\n",
    "    # =====================================================\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        state = self._get_state()\n",
    "        info = {\"checkpoints\": self.checkpoints_cleared}\n",
    "\n",
    "        return state, float(reward), done, info\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        sensor_distance, _ = ray_casting(self.car, self.track_surface)\n",
    "        left, front, right = sensor_distance\n",
    "        curvature = abs(left - right) / max(left + right, 1.0)\n",
    "        return np.array([left, front, right, self.car.speed, curvature], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        if not self.render_mode:\n",
    "            return\n",
    "        self.screen.blit(self.track_surface, (0, 0))\n",
    "        self.car.draw(self.screen)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=500,  save_path='d3qn.weights.h5'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import clear_output\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "    # History trackers\n",
    "    scores_history = []\n",
    "    avg_rewards = []\n",
    "    loss_history = []\n",
    "    max_speed_history = []\n",
    "    checkpoints_history = []\n",
    "    log_data = []\n",
    "    step_count=[]\n",
    "    time_per_episode=[]\n",
    "    best_avg_reward = -float(\"inf\")\n",
    "    no_improve_episodes = 0\n",
    "    plateau_patience = 50  # how long to wait before we “unstick” the agent\n",
    "    WARMUP_STEPS = 3000\n",
    "    state = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = np.argmax(agent.model.predict(np.expand_dims(state, axis=0), verbose=0)[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "    # Optional dynamic reward tweak (progress boost)\n",
    "    progress_boost = 1.0\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        current_time_start=time.time()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        episode_loss = []\n",
    "        episode_max_speed = 0\n",
    "        episode_checkpoints = getattr(env, \"checkpoints_cleared\", 0) if hasattr(env, \"checkpoints_cleared\") else 0\n",
    "        total_steps = 0\n",
    "        step=0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if \"checkpoints\" in info:\n",
    "                episode_checkpoints = info[\"checkpoints\"]\n",
    "            else:\n",
    "                episode_checkpoints = getattr(env, \"checkpoints_cleared\", episode_checkpoints)\n",
    "            reward_breakdown = info.get('reward_breakdown', {})\n",
    "            log_data.append({\n",
    "                'episode': episode,\n",
    "                'step': step,\n",
    "                'total_reward': reward,\n",
    "                'r_center': reward_breakdown.get('center', 0),\n",
    "                'r_speed': reward_breakdown.get('speed', 0),\n",
    "                'r_progress': reward_breakdown.get('progress', 0),\n",
    "                'r_step': reward_breakdown.get('step', 0),\n",
    "                'checkpoints': info.get('checkpoints', 0)\n",
    "            })\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            total_steps += 1\n",
    "            step += 1\n",
    "            if len(agent.memory) > WARMUP_STEPS and total_steps % 8 == 0:\n",
    "                agent.replay()\n",
    "            # Calculate loss proxy (difference between weights)\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                prev_weights = agent.model.get_weights()\n",
    "                # if total_steps % 4 == 0:\n",
    "                #     agent.replay()\n",
    "                \n",
    "\n",
    "                new_weights = agent.model.get_weights()\n",
    "                episode_loss.append(np.mean([np.mean(np.abs(n - p)) for n, p in zip(new_weights, prev_weights)]))\n",
    "\n",
    "            # Track metrics\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            episode_max_speed = max(episode_max_speed, getattr(env.car, \"speed\", 0))\n",
    "            if hasattr(env, \"checkpoints_cleared\"):\n",
    "                episode_checkpoints = env.checkpoints_cleared\n",
    "\n",
    "            current_time_end=time.time()-current_time_start\n",
    "\n",
    "        # Store histories\n",
    "        step_count.append(total_steps)\n",
    "        scores_history.append(total_reward)\n",
    "        loss_history.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        max_speed_history.append(episode_max_speed)\n",
    "        checkpoints_history.append(episode_checkpoints)\n",
    "        avg_rewards.append(np.mean(scores_history[-50:]))\n",
    "        time_per_episode.append(current_time_end)\n",
    "        \n",
    "        # --- Plateau detection logic ---\n",
    "        current_avg = avg_rewards[-1]\n",
    "        if current_avg > best_avg_reward:\n",
    "            best_avg_reward = current_avg\n",
    "            no_improve_episodes = 0\n",
    "        else:\n",
    "            no_improve_episodes += 1\n",
    "\n",
    "        if no_improve_episodes >= plateau_patience:\n",
    "            # Trigger recovery behavior\n",
    "            print(\"\\nPlateau detected! Boosting exploration and reward weights.\")\n",
    "            agent.epsilon = min(agent.epsilon + 0.2, 0.8)\n",
    "            progress_boost = min(progress_boost + 0.2, 2.0)\n",
    "            no_improve_episodes = 0  # reset counter\n",
    "\n",
    "\n",
    "        # Update graphs every 5 episodes\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(12, 22))\n",
    "\n",
    "            # --- Graph 1: Score ---\n",
    "            ax1.set_title('Agent Score Over Time')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Total Reward (Score)')\n",
    "            ax1.plot(scores_history, label='Score per Episode', color='royalblue')\n",
    "            ax1.plot(avg_rewards, label='50-Episode Average', color='orange', linestyle='--')\n",
    "            ax1.legend()\n",
    "\n",
    "            # --- Graph 2: Max Speed ---\n",
    "            ax2.set_title('Max Speed Achieved per Episode')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Max Speed')\n",
    "            ax2.plot(max_speed_history, label='Max Speed', color='purple')\n",
    "            ax2.legend()\n",
    "\n",
    "            # --- Graph 3: Model Loss ---\n",
    "            ax3.set_title('Episode Steps')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Steps per episode')\n",
    "            ax3.plot(step_count, label='Steps per Episode', color='green', alpha=0.7)\n",
    "            ax3.legend()\n",
    "\n",
    "            # --- Graph 4: Checkpoints Cleared ---\n",
    "            ax4.set_title('Checkpoints Cleared per Episode')\n",
    "            ax4.set_xlabel('Episode')\n",
    "            ax4.set_ylabel('Checkpoints Cleared')\n",
    "            episodes_range = range(len(checkpoints_history))\n",
    "            ax4.bar(episodes_range, checkpoints_history, color='forestgreen', label='Checkpoints')\n",
    "            ax4.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "            ax4.legend()\n",
    "\n",
    "            # --- Graph 5: Time per Episode ---\n",
    "            ax5.set_title('Time Taken per Episode')\n",
    "            ax5.set_xlabel('Episodes')\n",
    "            ax5.set_ylabel('Time(Seconds)')\n",
    "            ax5.plot(time_per_episode, label='Time per Episode', color='goldenrod')\n",
    "            ax5.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Histogram\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.title('Distribution of Checkpoints Cleared')\n",
    "            plt.xlabel('Checkpoints Cleared')\n",
    "            plt.ylabel('Number of Episodes')\n",
    "            plt.hist(checkpoints_history, bins=range(max(checkpoints_history) + 2), align='left', rwidth=0.8)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"Episode {episode+1}/{episodes} | Reward: {total_reward:.2f} | Avg: {avg_rewards[-1]:.2f} | Time: {time_per_episode[-1]:.2f} | Max Speed: {episode_max_speed:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            agent.model.save_weights(save_path)\n",
    "            print(f\"Weights saved at episode {episode+1}\")\n",
    "        if (episode + 1) % 200 == 0:\n",
    "            agent.model.save_weights(f\"d3qn_ep{episode+1}.weights.h5\")\n",
    "\n",
    "        if not hasattr(train_agent, \"best_checkpoint_record\"):\n",
    "            train_agent.best_checkpoint_record = -1  # static variable to persist across episodes\n",
    "\n",
    "        current_checkpoints = episode_checkpoints\n",
    "\n",
    "        if current_checkpoints > train_agent.best_checkpoint_record:\n",
    "            train_agent.best_checkpoint_record = current_checkpoints\n",
    "            best_file = f\"best_d3qn_{current_checkpoints}checkpoints.weights.h5\"\n",
    "            agent.model.save_weights(best_file)\n",
    "            print(f\"New Record! Cleared {current_checkpoints} checkpoints — weights saved as {best_file}\")\n",
    "        pda.DataFrame(log_data).to_csv(\"reward_components_log.csv\", index=False)\n",
    "        # =========================================\n",
    "        # CURRICULUM EXPANSION LOGIC\n",
    "        # =========================================\n",
    "        if episode_checkpoints >= env.curriculum_max_checkpoint:\n",
    "            env.curriculum_max_checkpoint = min(\n",
    "                env.curriculum_max_checkpoint + 1,\n",
    "                env.total_checkpoints\n",
    "            )\n",
    "            print(f\"Curriculum expanded → now up to checkpoint {env.curriculum_max_checkpoint}\")\n",
    "\n",
    "    return scores_history, avg_rewards, loss_history, max_speed_history, checkpoints_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b2866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, episodes=10, render=True):\n",
    "    print(\"\\nStarting Evaluation Phase...\\n\")\n",
    "    total_scores = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(agent.model.predict(np.expand_dims(state, axis=0), verbose=0)[0])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        total_scores.append(total_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward:.2f} | Steps = {step}\")\n",
    "\n",
    "    print(f\"\\n Avg Reward over {episodes} episodes: {np.mean(total_scores):.2f}\")\n",
    "    return total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Yash\\OneDrive\\Desktop\\Neural Networks Car Project\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Starting training with Physics Update...\n",
      "Starting training...\n",
      "Episode 1/500 | Reward: -17.97 | Avg: -17.97 | Time: 269.63 | Max Speed: 2.15 | Epsilon: 1.000\n",
      "New Record! Cleared 0 checkpoints — weights saved as best_d3qn_0checkpoints.weights.h5\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  \n",
    "# Initialize Environment and Agent\n",
    "env = GameEnv()\n",
    "state_size = 5\n",
    "action_size = 4\n",
    "agent = D3QNAgent(state_size, action_size)\n",
    "\n",
    "# 1. LOAD WEIGHTS\n",
    "# Ensure the file name matches exactly what you have on disk\n",
    "# try:\n",
    "#     print(\"Loading pretrained weights...\")\n",
    "#     agent.model.load_weights('best_d3qn_19checkpoints.weights.h5')\n",
    "#     agent.target_model.set_weights(agent.model.get_weights())\n",
    "#     print(\"Weights loaded successfully!\")\n",
    "# except:\n",
    "#     print(\"Weight file not found! Starting from scratch.\")\n",
    "\n",
    "\n",
    "agent.epsilon = 1.0   \n",
    "agent.epsilon_decay = 0.9995\n",
    "agent.epsilon_min = 0.1\n",
    "# Lower learning rate slightly to prevent destroying the pretrained knowledge\n",
    "agent.model.compile(optimizer=optimizers.Adam(learning_rate=1e-4, clipnorm=1.0), loss='mse')\n",
    "\n",
    "# 3. START TRAINING\n",
    "print(\"Starting training with Physics Update...\")\n",
    "train_agent(env, agent, episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv(render_mode=False)  # headless mode\n",
    "agent = D3QNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "\n",
    "test_scores = test_agent(env, agent, episodes=5, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758e633",
   "metadata": {},
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  \n",
    "\n",
    "env = GameEnv()  \n",
    "\n",
    "# get proper input/output sizes\n",
    "state = env.reset()\n",
    "state_size = len(state)\n",
    "action_size = 4  # left, right, brake\n",
    "\n",
    "# initialize agent\n",
    "agent = D3QNAgent(state_size=state_size, action_size=action_size)\n",
    "agent.model.load_weights('best_d3qn_19checkpoints.weights.h5')\n",
    "agent.target_model.set_weights(agent.model.get_weights())\n",
    "\n",
    "# Use very low epsilon to focus on learned policy\n",
    "agent.epsilon = 0.1 \n",
    "agent.epsilon_min = 0.01\n",
    "agent.epsilon_decay = 0.9999 \n",
    "\n",
    "# train\n",
    "rewards, avg, losses, max_speed, checkpoints = train_agent(env, agent, episodes=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
