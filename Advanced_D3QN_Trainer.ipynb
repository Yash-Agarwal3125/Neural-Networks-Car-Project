{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model, layers, optimizers\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pygame\n",
    "import numpy as np\n",
    "from Core_Game_Parts import *\n",
    "import os \n",
    "import pandas as pda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef28c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dueling_dqn(input_shape, action_size):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Value stream\n",
    "    value = layers.Dense(64, activation='relu')(x)\n",
    "    value = layers.Dense(1, activation='linear')(value)\n",
    "\n",
    "    # Advantage stream\n",
    "    advantage = layers.Dense(64, activation='relu')(x)\n",
    "    advantage = layers.Dense(action_size, activation='linear')(advantage)\n",
    "\n",
    "    # Combine value and advantage\n",
    "    q_values = layers.Lambda(lambda a: a[0] + (a[1] - tf.reduce_mean(a[1], axis=1, keepdims=True)))([value, advantage])\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=q_values)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "    return model\n",
    "class PERMemory:\n",
    "    def __init__(self, capacity, alpha=0.5):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def add(self, experience, td_error):\n",
    "        priority = (abs(td_error) + self.epsilon) ** self.alpha\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities / np.sum(priorities)\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return experiences, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for i, td_error in zip(indices, td_errors):\n",
    "            self.priorities[i] = (abs(td_error) + self.epsilon) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "class D3QNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.tau = 0.005\n",
    "        self.memory = PERMemory(20000)\n",
    "\n",
    "        self.model = build_dueling_dqn((state_size,), action_size)\n",
    "        self.target_model = build_dueling_dqn((state_size,), action_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        q_values = self.model.predict(np.expand_dims(state, 0), verbose=0)[0]\n",
    "        target_q = self.target_model.predict(np.expand_dims(next_state, 0), verbose=0)[0]\n",
    "        best_next_action = np.argmax(self.model.predict(np.expand_dims(next_state, 0), verbose=0)[0])\n",
    "        target = reward + self.gamma * target_q[best_next_action] * (1 - int(done))\n",
    "        td_error = target - q_values[action]\n",
    "        self.memory.add((state, action, reward, next_state, done), td_error)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch, indices, weights = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        next_qs = self.model.predict(next_states, verbose=0)\n",
    "        next_q_targets = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        td_errors = []\n",
    "        for i in range(self.batch_size):\n",
    "            best_action = np.argmax(next_qs[i])\n",
    "            target_value = rewards[i] + self.gamma * next_q_targets[i][best_action] * (1 - dones[i])\n",
    "            td_error = target_value - targets[i][actions[i]]\n",
    "            td_errors.append(td_error)\n",
    "            targets[i][actions[i]] += 0.1 * td_error\n",
    "\n",
    "        self.model.fit(states, targets, sample_weight=weights, epochs=1, verbose=0)\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        new_weights = []\n",
    "        for w, target_w in zip(self.model.get_weights(), self.target_model.get_weights()):\n",
    "            new_weights.append(self.tau * w + (1 - self.tau) * target_w)\n",
    "        self.target_model.set_weights(new_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, render_mode=False):\n",
    "        # Disable rendering (headless)\n",
    "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "        pygame.init()\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "        screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        self.track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "        self.car = Car(CAR_IMAGE_PATH, 900, 426, angle=-45)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        self.state_size = 4   # 3 sensors + speed\n",
    "        self.action_size = 3  # left, straight, right\n",
    "        self.max_steps = 2000\n",
    "        self.checkpoints_cleared = 0\n",
    "        self.prev_dist_to_next_checkpoint = None\n",
    "        self.current_checkpoint_idx = 0\n",
    "\n",
    "        if self.render_mode:\n",
    "            self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        else:\n",
    "            self.screen = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "    def reset(self):\n",
    "        self.car = Car(CAR_IMAGE_PATH, 900, 426, angle=-45)\n",
    "        self.steps = 0\n",
    "        self.checkpoints_cleared = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Steering\n",
    "        if action == 0:\n",
    "            self.car.angle += 5\n",
    "        elif action == 2:\n",
    "            self.car.angle -= 5\n",
    "        \n",
    "        # Move forward\n",
    "        self.car.speed = min(self.car.speed + 0.05, 5)\n",
    "        self.car.move()\n",
    "        sensor_distance, _ = ray_casting(self.car, self.track_surface)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Reward: distance & checkpoints (if applicable)\n",
    "        center_sensor = sensor_distance[1]\n",
    "        reward = (200 - abs(center_sensor - 100)) / 200.0\n",
    "        # ============================================\n",
    "        # ðŸ§  New Reward System\n",
    "# ============================================\n",
    "\n",
    "        # 1ï¸âƒ£ Centering Reward â€” encourages staying near track center\n",
    "        center_sensor = sensor_distance[1]  # middle ray distance\n",
    "        max_sensor = 200.0\n",
    "        r_center = (max_sensor - abs(center_sensor - 100.0)) / max_sensor  # normalized ~0..1\n",
    "        # 2ï¸âƒ£ Speed Reward â€” small incentive to move forward\n",
    "        MAX_SPEED = 5.0\n",
    "        r_speed = self.car.speed / MAX_SPEED  # normalized 0..1\n",
    "        # 3ï¸âƒ£ Progress Reward â€” reward moving toward next checkpoint\n",
    "        target_rect = checkpoint_data[self.current_checkpoint_idx]\n",
    "        target_x = target_rect[0] + target_rect[2] / 2\n",
    "        target_y = target_rect[1] + target_rect[3] / 2\n",
    "        curr_dist = np.hypot(self.car.x - target_x, self.car.y - target_y)\n",
    "\n",
    "# 2ï¸âƒ£ Speed Reward â€” small incentive to move forward\n",
    "        MAX_SPEED = 5.0\n",
    "        r_speed = self.car.speed / MAX_SPEED  # normalized 0..1\n",
    "\n",
    "# 3ï¸âƒ£ Progress Reward â€” reward moving toward next checkpoint\n",
    "        target_rect = checkpoint_data[self.current_checkpoint_idx]\n",
    "        target_x = target_rect[0] + target_rect[2] / 2\n",
    "        target_y = target_rect[1] + target_rect[3] / 2\n",
    "        curr_dist = np.hypot(self.car.x - target_x, self.car.y - target_y)\n",
    "\n",
    "        if self.prev_dist_to_next_checkpoint is None:\n",
    "            self.prev_dist_to_next_checkpoint = curr_dist\n",
    "\n",
    "# positive if car moved closer\n",
    "        r_progress = (self.prev_dist_to_next_checkpoint - curr_dist) / max_sensor\n",
    "        self.prev_dist_to_next_checkpoint = curr_dist\n",
    "\n",
    "# Bonus if checkpoint reached\n",
    "        checkpoint_rect = pygame.Rect(target_rect[0], target_rect[1], target_rect[2], target_rect[3])\n",
    "        if self.car.get_rect().colliderect(checkpoint_rect):\n",
    "            r_progress += 1.0\n",
    "            self.current_checkpoint_idx = (self.current_checkpoint_idx + 1) % len(checkpoint_data)\n",
    "            new_target = checkpoint_data[self.current_checkpoint_idx]\n",
    "            new_x = new_target[0] + new_target[2] / 2\n",
    "            new_y = new_target[1] + new_target[3] / 2\n",
    "            self.prev_dist_to_next_checkpoint = np.hypot(self.car.x - new_x, self.car.y - new_y)\n",
    "\n",
    "# 4ï¸âƒ£ Step Penalty â€” discourages standing still or farming reward\n",
    "        \n",
    "        \n",
    "        r_step = -0.02  # Slightly higher step penalty\n",
    "        crash_penalty = -3.0  # Add just before returning reward\n",
    "        w_c, w_s, w_p, w_step = 0.3, 0.05, 1.0, 1.0\n",
    "\n",
    "        reward = (w_c * r_center) + (w_s * r_speed) + (w_p * r_progress) + (w_step * r_step)\n",
    "\n",
    "# Clip reward to safe range\n",
    "        reward = float(np.clip(reward, -5.0, 5.0))\n",
    "\n",
    "        # Detect collisions via color\n",
    "        pixel_color = self.track_surface.get_at((int(self.car.x), int(self.car.y)))[0:3]\n",
    "        done = pixel_color == (50, 50, 50) or self.steps >= self.max_steps\n",
    "        if done:\n",
    "            reward += crash_penalty\n",
    "\n",
    "        # Optional: checkpoint logic (if you want to track)\n",
    "        # For now, just fake gradual progress scaling by steps\n",
    "        self.checkpoints_cleared = min(int(self.steps / 200), len(checkpoint_data))\n",
    "\n",
    "        state = self._get_state()\n",
    "        info = {\n",
    "        \"reward_breakdown\": {\n",
    "            \"center\": r_center,\n",
    "            \"speed\": r_speed,\n",
    "            \"progress\": r_progress,\n",
    "            \"step\": r_step\n",
    "        },\n",
    "        \"checkpoints\": self.current_checkpoint_idx\n",
    "        }\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        sensor_distance, _ = ray_casting(self.car, self.track_surface)\n",
    "        return np.array(sensor_distance + [self.car.speed], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        if not self.render_mode:\n",
    "            return\n",
    "        self.screen.blit(self.track_surface, (0, 0))\n",
    "        self.car.draw(self.screen)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=500,  save_path='d3qn.weights.h5'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import clear_output\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "    # History trackers\n",
    "    scores_history = []\n",
    "    avg_rewards = []\n",
    "    loss_history = []\n",
    "    max_speed_history = []\n",
    "    checkpoints_history = []\n",
    "    log_data = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        episode_loss = []\n",
    "        episode_max_speed = 0\n",
    "        episode_checkpoints = getattr(env, \"checkpoints_cleared\", 0) if hasattr(env, \"checkpoints_cleared\") else 0\n",
    "        total_steps = 0\n",
    "        step=0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            reward_breakdown = info.get('reward_breakdown', {})\n",
    "            log_data.append({\n",
    "                'episode': episode,\n",
    "                'step': step,\n",
    "                'total_reward': reward,\n",
    "                'r_center': reward_breakdown.get('center', 0),\n",
    "                'r_speed': reward_breakdown.get('speed', 0),\n",
    "                'r_progress': reward_breakdown.get('progress', 0),\n",
    "                'r_step': reward_breakdown.get('step', 0),\n",
    "                'checkpoints': info.get('checkpoints', 0)\n",
    "            })\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            total_steps += 1\n",
    "            step += 1\n",
    "\n",
    "            # Calculate loss proxy (difference between weights)\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                prev_weights = agent.model.get_weights()\n",
    "                # if total_steps % 4 == 0:\n",
    "                #     agent.replay()\n",
    "                if len(agent.memory) > agent.batch_size and np.random.rand() < 0.25:\n",
    "                    agent.replay()\n",
    "\n",
    "                new_weights = agent.model.get_weights()\n",
    "                episode_loss.append(np.mean([np.mean(np.abs(n - p)) for n, p in zip(new_weights, prev_weights)]))\n",
    "\n",
    "            # Track metrics\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_max_speed = max(episode_max_speed, getattr(env.car, \"speed\", 0))\n",
    "            if hasattr(env, \"checkpoints_cleared\"):\n",
    "                episode_checkpoints = env.checkpoints_cleared\n",
    "\n",
    "        # Store histories\n",
    "        scores_history.append(total_reward)\n",
    "        loss_history.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        max_speed_history.append(episode_max_speed)\n",
    "        checkpoints_history.append(episode_checkpoints)\n",
    "        avg_rewards.append(np.mean(scores_history[-50:]))\n",
    "\n",
    "        # Update graphs every 5 episodes\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 22))\n",
    "\n",
    "            # --- Graph 1: Score ---\n",
    "            ax1.set_title('Agent Score Over Time')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Total Reward (Score)')\n",
    "            ax1.plot(scores_history, label='Score per Episode', color='royalblue')\n",
    "            ax1.plot(avg_rewards, label='50-Episode Average', color='orange', linestyle='--')\n",
    "            ax1.legend()\n",
    "\n",
    "            # --- Graph 2: Max Speed ---\n",
    "            ax2.set_title('Max Speed Achieved per Episode')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Max Speed')\n",
    "            ax2.plot(max_speed_history, label='Max Speed', color='purple')\n",
    "            ax2.legend()\n",
    "\n",
    "            # --- Graph 3: Model Loss ---\n",
    "            ax3.set_title('Model Update Magnitude (Loss Proxy)')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Avg Model Weight Change')\n",
    "            ax3.plot(loss_history, label='Loss Proxy', color='orangered', alpha=0.7)\n",
    "            ax3.legend()\n",
    "\n",
    "            # --- Graph 4: Checkpoints Cleared ---\n",
    "            ax4.set_title('Checkpoints Cleared per Episode')\n",
    "            ax4.set_xlabel('Episode')\n",
    "            ax4.set_ylabel('Checkpoints Cleared')\n",
    "            episodes_range = range(len(checkpoints_history))\n",
    "            ax4.bar(episodes_range, checkpoints_history, color='forestgreen', label='Checkpoints')\n",
    "            ax4.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "            ax4.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Histogram\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.title('Distribution of Checkpoints Cleared')\n",
    "            plt.xlabel('Checkpoints Cleared')\n",
    "            plt.ylabel('Number of Episodes')\n",
    "            plt.hist(checkpoints_history, bins=range(max(checkpoints_history) + 2), align='left', rwidth=0.8)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"Episode {episode+1}/{episodes} | Reward: {total_reward:.2f} | Avg: {avg_rewards[-1]:.2f} | Loss: {loss_history[-1]:.6f} | Max Speed: {episode_max_speed:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            agent.model.save_weights(save_path)\n",
    "            print(f\"Weights saved at episode {episode+1}\")\n",
    "        if (episode + 1) % 200 == 0:\n",
    "            agent.model.save_weights(f\"d3qn_ep{episode+1}.weights.h5\")\n",
    "\n",
    "        if not hasattr(train_agent, \"best_checkpoint_record\"):\n",
    "            train_agent.best_checkpoint_record = -1  # static variable to persist across episodes\n",
    "\n",
    "        current_checkpoints = episode_checkpoints\n",
    "\n",
    "        if current_checkpoints > train_agent.best_checkpoint_record:\n",
    "            train_agent.best_checkpoint_record = current_checkpoints\n",
    "            best_file = f\"best_d3qn_{current_checkpoints}checkpoints.weights.h5\"\n",
    "            agent.model.save_weights(best_file)\n",
    "            print(f\"New Record! Cleared {current_checkpoints} checkpoints â€” weights saved as {best_file}\")\n",
    "        pda.DataFrame(log_data).to_csv(\"reward_components_log.csv\", index=False)\n",
    "    return scores_history, avg_rewards, loss_history, max_speed_history, checkpoints_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, episodes=10, render=True):\n",
    "    print(\"\\nStarting Evaluation Phase...\\n\")\n",
    "    total_scores = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(agent.model.predict(np.expand_dims(state, axis=0), verbose=0)[0])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        total_scores.append(total_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward:.2f} | Steps = {step}\")\n",
    "\n",
    "    print(f\"\\n Avg Reward over {episodes} episodes: {np.mean(total_scores):.2f}\")\n",
    "    return total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  \n",
    "\n",
    "env = GameEnv()  \n",
    "\n",
    "# get proper input/output sizes\n",
    "state = env.reset()\n",
    "state_size = len(state)\n",
    "action_size = 3  # left, right, brake\n",
    "\n",
    "# initialize agent\n",
    "agent = D3QNAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "# train\n",
    "rewards, avg, losses, max_speed, checkpoints = train_agent(env, agent, episodes=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv(render_mode=False)  # headless mode\n",
    "agent = D3QNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "\n",
    "test_scores = test_agent(env, agent, episodes=5, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
