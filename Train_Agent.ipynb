{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pygame\n",
    "from Core_Game_Parts import *\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os         \n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \"\"\"Builds a simple feedforward neural network model.\n",
    "\n",
    "    Returns:\n",
    "        model: A Keras Sequential model instance.\n",
    "    \"\"\"\n",
    "    model=Sequential(\n",
    "        [Dense(32,activation='relu',input_shape=(4,)),\n",
    "         Dense(16,activation='relu'),\n",
    "         Dense(3,activation='linear')]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "ai_model=model()\n",
    "ai_model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    A Deep Q-Network (DQN) agent for reinforcement learning.\n",
    "    Attributes:\n",
    "        model: The neural network model used for approximating Q-values.\n",
    "        memory: A deque to store past experiences for experience replay.\n",
    "        gamma: Discount factor for future rewards.\n",
    "        epsilon: Exploration rate for the epsilon-greedy policy.\n",
    "        epsilon_min: Minimum exploration rate.\n",
    "        epsilon_decay: Decay rate for exploration after each training episode.\n",
    "        batch_size: Size of the minibatch for training.\n",
    "    Methods:\n",
    "        remember: Store an experience in memory.\n",
    "        choose_action: Select an action based on the current state using an epsilon-greedy policy.\n",
    "        train_from_memory: Train the model using a minibatch of experiences from memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.memory = deque(maxlen=20000) # Increased memory size for better learning\n",
    "        self.target_model = tf.keras.models.clone_model(self.model) # Target model for stability\n",
    "        self.update_target_model() # Initialize target model\n",
    "        self.gamma = 0.95  # Discount factor: how much to value future rewards\n",
    "        self.epsilon = 1.0  # Exploration rate: initial probability of taking a random action\n",
    "        self.epsilon_min = 0.01 # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.999  # Decay rate for exploration\n",
    "        self.batch_size = 128 # Increased batch size for more stable training\n",
    "        self.target_update_counter = 0 # Counter to track when to update the target model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copies the weights from the main model to the target model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores an experience tuple in the agent's memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state using an epsilon-greedy policy.\n",
    "        With probability epsilon, it takes a random action (exploration).\n",
    "        Otherwise, it takes the best known action (exploitation).\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(3)  # Return a random action (0, 1, 2)\n",
    "        \n",
    "        # Predict Q-values for the given state and choose the action with the highest Q-value\n",
    "        q_values = self.model.predict(np.reshape(state, [1, 4]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train_from_memory(self):\n",
    "        \"\"\"Train the DQN agent using experiences from memory.\n",
    "\n",
    "        Returns:\n",
    "            float: The training loss.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None # Return None if not training\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([experience[0] for experience in minibatch])\n",
    "        actions = np.array([experience[1] for experience in minibatch])\n",
    "        rewards = np.array([experience[2] for experience in minibatch])\n",
    "        next_states = np.array([experience[3] for experience in minibatch])\n",
    "        dones = np.array([experience[4] for experience in minibatch])\n",
    "\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        targets = rewards + self.gamma * np.amax(next_q_values, axis=1) * (1 - dones)\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            current_q_values[i][action] = targets[i]\n",
    "\n",
    "        # FIX: Capture the history object to get the loss\n",
    "        history = self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ed7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_game_step(action, car, track_image, current_checkpoint):\n",
    "    \"\"\"\n",
    "    Simulates a game step for the car in the racing environment.\n",
    "\n",
    "    Args:\n",
    "        action (int): Action to be taken by the car (0: left, 1: straight, 2: right, 3: brake).\n",
    "        car (Car): The car object representing the player's car.\n",
    "        track_image (Surface): The image of the track.\n",
    "        current_checkpoint (int): The index of the current checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the new state, done flag, reward, and current checkpoint.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    \n",
    "    # --- NEW MULTI-COMPONENT REWARD SYSTEM ---\n",
    "\n",
    "    # 1. Base reward: A small penalty for each step taken. \n",
    "    # This encourages the agent to finish the lap faster.\n",
    "    reward = -0.5\n",
    "    \n",
    "    if car.speed < 0.3:\n",
    "        reward -= 2  # Extra penalty for being too slow\n",
    "\n",
    "    # 2. Reward for Speed: Encourage the car to move forward, not stand still.\n",
    "    # The reward is proportional to its speed.\n",
    "    reward += car.speed * 0.5\n",
    "\n",
    "    # 3. Reward for Progress: This is the most important part.\n",
    "    # We get the sensor readings (the state) and reward the agent\n",
    "    # for having a clear path ahead. The middle sensor (state[1]) looks forward.\n",
    "    current_state, _ = ray_casting(car, track_image)\n",
    "    # The farther the wall, the higher the reward.\n",
    "    reward += current_state[1] * 0.01\n",
    "    \n",
    "    is_turning = action == 0 or action == 1\n",
    "    is_braking = action == 2\n",
    "    if is_turning and car.speed > (MAX_SPEED * 0.5): # Encourage braking only at high speeds.\n",
    "        if is_braking:\n",
    "            reward += 1.0 # Reward for braking in a turn.\n",
    "\n",
    "    # 4. Penalty for Sharp Turns: Discourage frantic wiggling.\n",
    "    # Encourage smoother driving by penalizing turning actions slightly.\n",
    "    if action == 0: # Actions for left turns\n",
    "        reward -= 0.2\n",
    "\n",
    "    # --- CAR PHYSICS (No changes here) ---\n",
    "    car.speed += ACCELERATION\n",
    "    if car.speed > 0:\n",
    "        speed_factor = car.speed / MAX_SPEED\n",
    "        dynamic_turn_angle = MAX_TURN_ANGLE - (speed_factor) * (MAX_TURN_ANGLE - MIN_TURN_ANGLE)\n",
    "        if action == 0:  # Left\n",
    "            car.angle += dynamic_turn_angle\n",
    "        elif action == 1:  # Right\n",
    "            car.angle -= dynamic_turn_angle\n",
    "    \n",
    "    if action == 2: # Brake\n",
    "        car.speed -= BRAKE_FORCE\n",
    "    \n",
    "    car.speed -= FRICTION\n",
    "    car.speed = max(0, min(car.speed, MAX_SPEED))\n",
    "    car.move()\n",
    "    \n",
    "    # --- GOAL-BASED REWARDS (Checkpoints and Crashing) ---\n",
    "    checkpoint_rects = [pygame.Rect(x, y, w, h) for x, y, w, h, a in checkpoint_data]\n",
    "    \n",
    "    # 5. Large reward for hitting a checkpoint.\n",
    "    if current_checkpoint < len(checkpoint_rects):\n",
    "        if car.rect.colliderect(checkpoint_rects[current_checkpoint]):\n",
    "            current_checkpoint += 1\n",
    "            reward += 200 # Large positive reward\n",
    "            print(f\"Checkpoint {current_checkpoint} reached!\")\n",
    "\n",
    "    # 6. Very large reward for finishing the lap.\n",
    "    if current_checkpoint == len(checkpoint_rects) and car.rect.colliderect(finish_line_rect):\n",
    "        reward += 1000\n",
    "        current_checkpoint = 0\n",
    "        print(\"Lap finished!\")\n",
    "\n",
    "    # 7. Large penalty for crashing.\n",
    "    try:\n",
    "        pixel_color = track_image.get_at((int(car.x), int(car.y)))[:3]\n",
    "        if pixel_color == DRAW_COLOR:\n",
    "            done = True\n",
    "    except IndexError:\n",
    "        done = True\n",
    "        \n",
    "    if done:\n",
    "        reward = -100 # Keep a significant penalty for crashing, but not as extreme as -100\n",
    "\n",
    "    new_state, _ = ray_casting(car, track_image)\n",
    "    return new_state, done, reward, current_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa55220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=500):\n",
    "    \"\"\"\n",
    "    Main function to train the DQN agent with enhanced logging and plotting.\n",
    "    \"\"\"\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT)) # Keep commented out for speed\n",
    "    track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "    \n",
    "    ai_model = model()\n",
    "    agent = DQNAgent(ai_model)\n",
    "\n",
    "    # NEW: Add lists to store history for new plots\n",
    "    scores_history = []\n",
    "    loss_history = []\n",
    "    checkpoints_history = []\n",
    "    max_speed_history = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        car = Car(CAR_IMAGE_PATH, DEFAULT_START_X, DEFAULT_START_Y, DEFAULT_START_ANGLE, DEFAULT_START_SPEED)\n",
    "        current_checkpoint = 0\n",
    "        \n",
    "        distances, _ = ray_casting(car, track_surface)\n",
    "        speed = car.speed / MAX_SPEED \n",
    "        state = np.array(distances + [speed]) \n",
    "        state = np.reshape(state, [1, 4])\n",
    "        \n",
    "        total_reward = 0\n",
    "        max_steps_per_episode = 2000\n",
    "        max_speed_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = agent.choose_action(state)\n",
    "            distances_next, done, reward, new_checkpoint = model_game_step(action, car, track_surface, current_checkpoint)\n",
    "            \n",
    "            max_speed_episode = max(max_speed_episode, car.speed)\n",
    "            \n",
    "            total_reward += reward\n",
    "            speed_next = car.speed / MAX_SPEED \n",
    "            next_state = np.array(distances_next + [speed_next])\n",
    "            next_state = np.reshape(next_state, [1, 4])\n",
    "            current_checkpoint = new_checkpoint\n",
    "            \n",
    "            agent.remember(state[0], action, reward, next_state[0], done)\n",
    "            state = next_state\n",
    "            \n",
    "            if step % 4 == 0: # Train every 8 steps for speed\n",
    "                loss = agent.train_from_memory()\n",
    "                if loss is not None:\n",
    "                    loss_history.append(loss)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if e % 5 == 0:\n",
    "            agent.update_target_model()\n",
    "            print(f\"--- Target Network Updated at Episode {e+1} ---\")\n",
    "            \n",
    "        # NEW: Append the new metrics to their history lists\n",
    "        scores_history.append(total_reward)\n",
    "        checkpoints_history.append(current_checkpoint)\n",
    "        max_speed_history.append(max_speed_episode)\n",
    "        \n",
    "        print(\n",
    "            f\"Episode: {e+1}/{episodes}, \"\n",
    "            f\"Score: {total_reward:.2f}, \"\n",
    "            f\"Max Speed: {max_speed_episode:.2f}, \"\n",
    "            f\"Checkpoints: {current_checkpoint}, \"\n",
    "            f\"Epsilon: {agent.epsilon:.2f}\"\n",
    "        )\n",
    "\n",
    "        if (e + 1) % 50 == 0:\n",
    "            ai_model.save_weights(f\"dqn_car_weights_episode_{e+1}.weights.h5\")\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "# --- MODIFIED: Enhanced plotting section with Bar Chart ---\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # NEW: Create 4 subplots instead of 3, and increase the figure size\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 22)) \n",
    "\n",
    "    # --- Graph 1: Score ---\n",
    "    ax1.set_title('Agent Score Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward (Score)')\n",
    "    ax1.plot(scores_history, label='Score per Episode', color='royalblue')\n",
    "    ax1.legend()\n",
    "\n",
    "    # --- Graph 2: Max Speed ---\n",
    "    ax2.set_title('Max Speed Achieved per Episode')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Max Speed')\n",
    "    ax2.plot(max_speed_history, label='Max Speed', color='purple')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # --- Graph 3: Model Loss ---\n",
    "    ax3.set_title('Model Loss Over Time')\n",
    "    ax3.set_xlabel('Training Step')\n",
    "    ax3.set_ylabel('MSE Loss')\n",
    "    ax3.plot(loss_history, label='Training Loss', color='orangered', alpha=0.7)\n",
    "    ax3.legend()\n",
    "\n",
    "    # --- NEW: Graph 4: Checkpoints Bar Chart ---\n",
    "    ax4.set_title('Checkpoints Cleared per Episode')\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Checkpoints Cleared')\n",
    "    # Use ax4.bar() to create the bar chart\n",
    "    episodes = range(len(checkpoints_history))\n",
    "    ax4.bar(episodes, checkpoints_history, color='forestgreen', label='Checkpoints')\n",
    "    # Set y-axis to be integers since you can't clear half a checkpoint\n",
    "    ax4.yaxis.set_major_locator(plt.MaxNLocator(integer=True)) \n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # --- Optional: Add a histogram for checkpoint distribution ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title('Distribution of Checkpoints Cleared')\n",
    "    plt.xlabel('Number of Checkpoints Cleared in an Episode')\n",
    "    plt.ylabel('Number of Episodes')\n",
    "    plt.hist(checkpoints_history, bins=range(max(checkpoints_history) + 2), align='left', rwidth=0.8)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61af01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    train_dqn(episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b2f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
