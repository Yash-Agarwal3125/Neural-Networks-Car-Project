{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f9f688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash\\OneDrive\\Desktop\\Neural Networks Car Project\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from Core_Game_Parts import *\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expert_data(filename=\"pretrain_data.npy\"):\n",
    "    \"\"\"\n",
    "    Run the simulation manually and save state-action pairs.\n",
    "    \"\"\"\n",
    "    # Force a display window to open for manual play\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"windows\"\n",
    "    \n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "    clock = pygame.time.Clock()\n",
    "    track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "    car = Car(CAR_IMAGE_PATH, DEFAULT_START_X, DEFAULT_START_Y, angle=DEFAULT_START_ANGLE)\n",
    "    \n",
    "    driving_data = []\n",
    "    running = True\n",
    "    print(\"Starting data collection. Drive 3-5 clean laps. Press ESC or close window to finish.\")\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):\n",
    "                running = False\n",
    "        \n",
    "        keys = pygame.key.get_pressed()\n",
    "        action = None\n",
    "        # Map keys to the agent's actions (0: Left, 1: Right, 2: Brake)\n",
    "        if keys[pygame.K_LEFT] or keys[pygame.K_a]:  action = 0\n",
    "        elif keys[pygame.K_RIGHT] or keys[pygame.K_d]: action = 1\n",
    "        elif keys[pygame.K_DOWN] or keys[pygame.K_s]:  action = 2\n",
    "\n",
    "        distances, _ = ray_casting(car, track_surface)\n",
    "        normalized_speed = car.speed / MAX_SPEED\n",
    "        state = np.array(distances + [normalized_speed])\n",
    "        \n",
    "        if action is not None:\n",
    "            driving_data.append([state, action])\n",
    "\n",
    "        # Standard manual driving physics\n",
    "        if keys[pygame.K_UP] or keys[pygame.K_w]: car.speed += ACCELERATION\n",
    "        if car.speed > 0:\n",
    "            speed_factor = car.speed / MAX_SPEED\n",
    "            turn = MAX_TURN_ANGLE - (speed_factor) * (MAX_TURN_ANGLE - MIN_TURN_ANGLE)\n",
    "            if keys[pygame.K_LEFT] or keys[pygame.K_a]: car.angle += turn\n",
    "            if keys[pygame.K_RIGHT] or keys[pygame.K_d]: car.angle -= turn\n",
    "        if keys[pygame.K_DOWN] or keys[pygame.K_s]: car.speed -= BRAKE_FORCE\n",
    "        car.speed -= FRICTION\n",
    "        car.speed = max(0, min(car.speed, MAX_SPEED))\n",
    "        car.move()\n",
    "\n",
    "        screen.blit(track_surface, (0, 0)); car.draw(screen); pygame.display.update(); clock.tick(60)\n",
    "\n",
    "    pygame.quit()\n",
    "    \n",
    "    if driving_data:\n",
    "        print(f\"Saving {len(driving_data)} data points to {filename}...\")\n",
    "        driving_data_array = np.array(driving_data, dtype=object)\n",
    "        np.save(filename, driving_data_array, allow_pickle=True)\n",
    "        print(\"Save complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb250a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \"\"\"Builds the neural network model.\"\"\"\n",
    "    net = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax') # Softmax is better for imitation learning\n",
    "    ])\n",
    "    net.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return net\n",
    "\n",
    "def pretrain_agent(data_path=\"pretrain_data.npy\", weights_path=\"pretrained.weights.h5\"):\n",
    "    \"\"\"\n",
    "    Trains a model on the expert data using supervised learning.\n",
    "    \"\"\"\n",
    "    print(\"Loading expert data...\")\n",
    "    expert_data = np.load(data_path, allow_pickle=True)\n",
    "    \n",
    "    states = np.array([item[0] for item in expert_data])\n",
    "    actions = np.array([item[1] for item in expert_data])\n",
    "    \n",
    "    # Convert actions to one-hot encoding (e.g., 0 -> [1,0,0], 1 -> [0,1,0])\n",
    "    actions_one_hot = to_categorical(actions, num_classes=3)\n",
    "    \n",
    "    print(f\"Data loaded. Training on {len(states)} samples...\")\n",
    "    ai_model = model()\n",
    "    \n",
    "    ai_model.fit(states, actions_one_hot, epochs=15, batch_size=64, validation_split=0.1, shuffle=True)\n",
    "    \n",
    "    print(f\"Pre-training complete. Saving weights to {weights_path}...\")\n",
    "    ai_model.save_weights(weights_path)\n",
    "    print(\"Weights saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad86c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imitation_model(data_path=\"pretrain_data.npy\", weights_path=\"final_imitation.weights.h5\"):\n",
    "    print(\"Loading expert data...\")\n",
    "    expert_data = np.load(data_path, allow_pickle=True)\n",
    "    \n",
    "    states = np.array([item[0] for item in expert_data])\n",
    "    actions = np.array([item[1] for item in expert_data])\n",
    "    actions_one_hot = to_categorical(actions, num_classes=3)\n",
    "    \n",
    "    print(f\"Data loaded. Training on {len(states)} samples...\")\n",
    "    \n",
    "    # Define the model for classification\n",
    "    imitation_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax') # Softmax for predicting the probability of each action\n",
    "    ])\n",
    "    imitation_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model to imitate\n",
    "    imitation_model.fit(states, actions_one_hot, epochs=25, batch_size=64, validation_split=0.1, shuffle=True)\n",
    "    \n",
    "    print(f\"Training complete. Saving final weights to {weights_path}...\")\n",
    "    imitation_model.save_weights(weights_path)\n",
    "    print(\"Final weights saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2790c5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert data...\n",
      "Data loaded. Training on 2790 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash\\OneDrive\\Desktop\\Neural Networks Car Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7551 - loss: 1.0004 - val_accuracy: 0.6380 - val_loss: 0.7810\n",
      "Epoch 2/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.4412 - val_accuracy: 0.7276 - val_loss: 0.4984\n",
      "Epoch 3/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7658 - loss: 0.4675 - val_accuracy: 0.6487 - val_loss: 0.9728\n",
      "Epoch 4/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7662 - loss: 0.4289 - val_accuracy: 0.7348 - val_loss: 0.5257\n",
      "Epoch 5/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4637 - val_accuracy: 0.7419 - val_loss: 0.4867\n",
      "Epoch 6/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7714 - loss: 0.4057 - val_accuracy: 0.7312 - val_loss: 0.4674\n",
      "Epoch 7/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7658 - loss: 0.4140 - val_accuracy: 0.7204 - val_loss: 0.4732\n",
      "Epoch 8/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7698 - loss: 0.4642 - val_accuracy: 0.7348 - val_loss: 0.4489\n",
      "Epoch 9/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7794 - loss: 0.4063 - val_accuracy: 0.7455 - val_loss: 0.6489\n",
      "Epoch 10/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7893 - loss: 0.3873 - val_accuracy: 0.7742 - val_loss: 0.4120\n",
      "Epoch 11/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7722 - loss: 0.3902 - val_accuracy: 0.8065 - val_loss: 0.4169\n",
      "Epoch 12/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7973 - loss: 0.3699 - val_accuracy: 0.7527 - val_loss: 0.4322\n",
      "Epoch 13/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7901 - loss: 0.3684 - val_accuracy: 0.7348 - val_loss: 0.4621\n",
      "Epoch 14/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7917 - loss: 0.3803 - val_accuracy: 0.7670 - val_loss: 0.4529\n",
      "Epoch 15/15\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7758 - loss: 0.4465 - val_accuracy: 0.7384 - val_loss: 0.6100\n",
      "Pre-training complete. Saving weights to pretrained.weights.h5...\n",
      "Weights saved!\n",
      "Loading expert data...\n",
      "Data loaded. Training on 2790 samples...\n",
      "Epoch 1/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6838 - loss: 1.2661 - val_accuracy: 0.6559 - val_loss: 0.9449\n",
      "Epoch 2/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7599 - loss: 0.4384 - val_accuracy: 0.6308 - val_loss: 0.7694\n",
      "Epoch 3/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7599 - loss: 0.4307 - val_accuracy: 0.6559 - val_loss: 0.7240\n",
      "Epoch 4/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.4051 - val_accuracy: 0.6667 - val_loss: 0.4918\n",
      "Epoch 5/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7603 - loss: 0.4332 - val_accuracy: 0.7312 - val_loss: 0.4923\n",
      "Epoch 6/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7658 - loss: 0.4239 - val_accuracy: 0.6559 - val_loss: 0.7079\n",
      "Epoch 7/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7790 - loss: 0.4068 - val_accuracy: 0.7634 - val_loss: 0.4317\n",
      "Epoch 8/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7782 - loss: 0.4181 - val_accuracy: 0.7455 - val_loss: 0.5053\n",
      "Epoch 9/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7929 - loss: 0.3882 - val_accuracy: 0.7563 - val_loss: 0.4970\n",
      "Epoch 10/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.3949 - val_accuracy: 0.7814 - val_loss: 0.4055\n",
      "Epoch 11/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7794 - loss: 0.4083 - val_accuracy: 0.7599 - val_loss: 0.4116\n",
      "Epoch 12/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.3840 - val_accuracy: 0.7491 - val_loss: 0.4016\n",
      "Epoch 13/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7921 - loss: 0.3815 - val_accuracy: 0.7634 - val_loss: 0.4582\n",
      "Epoch 14/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8037 - loss: 0.3682 - val_accuracy: 0.7455 - val_loss: 0.5430\n",
      "Epoch 15/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7814 - loss: 0.4378 - val_accuracy: 0.7670 - val_loss: 0.4082\n",
      "Epoch 16/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8009 - loss: 0.3659 - val_accuracy: 0.7742 - val_loss: 0.4140\n",
      "Epoch 17/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7885 - loss: 0.4040 - val_accuracy: 0.7133 - val_loss: 0.4755\n",
      "Epoch 18/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7873 - loss: 0.4030 - val_accuracy: 0.7670 - val_loss: 0.4309\n",
      "Epoch 19/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.4149 - val_accuracy: 0.7348 - val_loss: 0.4482\n",
      "Epoch 20/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7893 - loss: 0.3862 - val_accuracy: 0.7527 - val_loss: 0.4048\n",
      "Epoch 21/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.3794 - val_accuracy: 0.7527 - val_loss: 0.3966\n",
      "Epoch 22/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8029 - loss: 0.3746 - val_accuracy: 0.7527 - val_loss: 0.4106\n",
      "Epoch 23/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7794 - loss: 0.4040 - val_accuracy: 0.7348 - val_loss: 0.4392\n",
      "Epoch 24/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7985 - loss: 0.3759 - val_accuracy: 0.7599 - val_loss: 0.5093\n",
      "Epoch 25/25\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7865 - loss: 0.3862 - val_accuracy: 0.8029 - val_loss: 0.3935\n",
      "Training complete. Saving final weights to final_imitation.weights.h5...\n",
      "Final weights saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #collect_expert_data()  #for collecting the data\n",
    "    pretrain_agent()       #for pre training\n",
    "    train_imitation_model() #for Behavioral Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e59d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
