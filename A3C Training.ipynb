{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f85c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import multiprocessing as mp\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d72b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_WIDTH, SCREEN_HEIGHT = 1280, 720\n",
    "BG_COLOR = (120, 120, 120)  # The GREY color of the WALL/BACKGROUND\n",
    "DRAW_COLOR = (50, 50, 50)   # The DARK GREY color of the ROAD\n",
    "CAR_WIDTH, CAR_HEIGHT = 20, 40\n",
    "DEFAULT_START_X, DEFAULT_START_Y = 1050, 550 # A known SAFE starting position\n",
    "DEFAULT_START_ANGLE = 180 # Pointing left towards the first turn\n",
    "ACCELERATION = 0.05\n",
    "BRAKE_FORCE = 0.1\n",
    "MAX_SPEED = 5.00\n",
    "FRICTION = 0.025\n",
    "MIN_TURN_ANGLE = 1.5\n",
    "MAX_TURN_ANGLE = 2.0\n",
    "CAR_IMAGE_PATH = \"Track_images/car.png\"\n",
    "TRACK_IMAGE_PATH = \"Track_images/track1.png\"\n",
    "\n",
    "checkpoint_data = [\n",
    "    (834, 520, 10, 120, 0), (600, 540, 10, 120, 0), (110, 569, 10, 120, 90),\n",
    "    (285, 483, 10, 120, 0), (366, 314, 10, 120, 0), (355, 173, 10, 120, 0),\n",
    "    (450, 109, 10, 120, 0), (606, 170, 10, 120, 0), (818, 91, 10, 120, 0),\n",
    "    (1127, 88, 10, 120, 310), (1094, 270, 10, 120, 0), (920, 346, 10, 120, 45)\n",
    "]\n",
    "\n",
    "# --- CORE CLASSES AND FUNCTIONS (Corrected) ---\n",
    "class Car:\n",
    "    def __init__(self, image_path, x, y, angle=0, speed=0):\n",
    "        if pygame.display.get_init():\n",
    "             self.original_image = pygame.transform.scale(pygame.image.load(image_path).convert_alpha(), (CAR_WIDTH, CAR_HEIGHT))\n",
    "        else:\n",
    "             self.original_image = pygame.Surface((CAR_WIDTH, CAR_HEIGHT), pygame.SRCALPHA)\n",
    "        self.x, self.y, self.angle, self.speed = x, y, angle, speed\n",
    "        self.rect = self.original_image.get_rect(center=(self.x, self.y))\n",
    "    def move(self):\n",
    "        rad = np.radians(self.angle)\n",
    "        self.x += self.speed * np.cos(rad); self.y -= self.speed * np.sin(rad)\n",
    "        self.rect.center = (self.x, self.y)\n",
    "\n",
    "def ray_casting(car, track_surface):\n",
    "    distances = []\n",
    "    for angle in [-45, 0, 45]:\n",
    "        ray_angle, (ray_x, ray_y), dist = car.angle + angle, (car.x, car.y), 0\n",
    "        while dist < 200:\n",
    "            rad = np.radians(ray_angle)\n",
    "            ray_x += np.cos(rad); ray_y -= np.sin(rad)\n",
    "            dist += 1\n",
    "            if not (0 <= ray_x < SCREEN_WIDTH and 0 <= ray_y < SCREEN_HEIGHT): break\n",
    "            try:\n",
    "                # BUG FIX: Stop at the WALL (BG_COLOR)\n",
    "                if track_surface.get_at((int(ray_x), int(ray_y)))[:3] == BG_COLOR: break\n",
    "            except (IndexError, pygame.error): break\n",
    "        distances.append(dist)\n",
    "    return distances, []\n",
    "\n",
    "def model_game_step(action, car, track_surface, current_checkpoint):\n",
    "    done, reward = False, car.speed * 0.1\n",
    "    current_state, _ = ray_casting(car, track_surface)\n",
    "    reward += current_state[1] * 0.01\n",
    "    car.speed += ACCELERATION\n",
    "    if car.speed > 0:\n",
    "        turn = MAX_TURN_ANGLE - (car.speed/MAX_SPEED)*(MAX_TURN_ANGLE-MIN_TURN_ANGLE)\n",
    "        if action == 0: car.angle += turn\n",
    "        elif action == 1: car.angle -= turn\n",
    "    if action == 2: car.speed -= BRAKE_FORCE\n",
    "    car.speed -= FRICTION; car.speed = max(0, min(car.speed, MAX_SPEED)); car.move()\n",
    "    checkpoint_rects = [pygame.Rect(x,y,w,h) for x,y,w,h,a in checkpoint_data]\n",
    "    if current_checkpoint < len(checkpoint_rects):\n",
    "        if car.rect.colliderect(checkpoint_rects[current_checkpoint]):\n",
    "            current_checkpoint += 1; reward += 1000\n",
    "    try:\n",
    "        # BUG FIX: Crash on the WALL (BG_COLOR)\n",
    "        if track_surface.get_at((int(car.x), int(car.y)))[:3] == BG_COLOR: done = True\n",
    "    except (IndexError, pygame.error): done = True\n",
    "    if done: reward = -100\n",
    "    new_state, _ = ray_casting(car, track_surface)\n",
    "    return new_state, done, reward, current_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a2eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: ACTOR-CRITIC LOGIC & PLOTTING\n",
    "# ==============================================================================\n",
    "def model_fn():\n",
    "    net = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='linear')\n",
    "    ])\n",
    "    return net\n",
    "\n",
    "def run_actor(actor_id, experience_queue, weights_path, stop_event):\n",
    "    # --- FIX: Isolate this process to the CPU to prevent GPU deadlocks ---\n",
    "    import tensorflow as tf\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "    print(f\"[Actor {actor_id}] Started.\")\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "    pygame.init()\n",
    "\n",
    "    track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "    actor_model = model_fn()\n",
    "    epsilon = 0.1 + (random.random() * 0.9)\n",
    "    epsilon_min, epsilon_decay = 0.01, 0.9998\n",
    "    episode_count = 0\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        if episode_count % 10 == 0:\n",
    "            try:\n",
    "                actor_model.load_weights(weights_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        car = Car(CAR_IMAGE_PATH, DEFAULT_START_X, DEFAULT_START_Y, DEFAULT_START_ANGLE)\n",
    "        ckpt, total_r, max_s = 0, 0, 0\n",
    "        dist, _ = ray_casting(car, track_surface)\n",
    "        state = np.array(dist + [car.speed / MAX_SPEED])\n",
    "\n",
    "        for step in range(5000):\n",
    "            print(f\"[Actor {actor_id}] Episode {episode_count}, Step {step}: Predicting action...\")\n",
    "\n",
    "            action = random.randrange(3) if np.random.rand() <= epsilon else np.argmax(actor_model.predict(np.reshape(state, [1, 4]), verbose=0)[0])\n",
    "            dist_next, done, reward, new_ckpt = model_game_step(action, car, track_surface, ckpt)\n",
    "            next_state = np.array(dist_next + [car.speed / MAX_SPEED])\n",
    "\n",
    "            experience_queue.put(('experience', (state, action, reward, next_state, done)))\n",
    "\n",
    "            state, ckpt = next_state, new_ckpt\n",
    "            total_r += reward\n",
    "            max_s = max(max_s, car.speed)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        experience_queue.put(('summary', (total_r, ckpt, max_s)))\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        episode_count += 1\n",
    "\n",
    "    print(f\"[Actor {actor_id}] Stopping.\")\n",
    "\n",
    "def run_learner(experience_queue, weights_path, stop_event, total_episodes):\n",
    "    print(\"[Learner] Started. Initializing...\")\n",
    "    learner_model = model_fn()\n",
    "    learner_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
    "    target_model = model_fn()\n",
    "    target_model.set_weights(learner_model.get_weights())\n",
    "    learner_model.save_weights(weights_path)\n",
    "    \n",
    "    memory = deque(maxlen=50000)\n",
    "    batch_size = 256\n",
    "    episodes_completed, train_steps = 0, 0\n",
    "    history = {'scores': [], 'checkpoints': [], 'max_speed': [], 'loss': []}\n",
    "\n",
    "    print(\"[Learner] Ready. Waiting for data...\")\n",
    "    while episodes_completed < total_episodes:\n",
    "        while not experience_queue.empty():\n",
    "            msg_type, data = experience_queue.get()\n",
    "            if msg_type == 'experience': memory.append(data)\n",
    "            elif msg_type == 'summary':\n",
    "                score, ckpt, max_speed = data\n",
    "                history['scores'].append(score); history['checkpoints'].append(ckpt); history['max_speed'].append(max_speed)\n",
    "                episodes_completed += 1\n",
    "        \n",
    "        if len(memory) < batch_size * 4:\n",
    "            time.sleep(0.1); continue\n",
    "            \n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        \n",
    "        states_list, actions_list, rewards_list, next_states_list, dones_list = [], [], [], [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            states_list.append(state); actions_list.append(action); rewards_list.append(reward)\n",
    "            next_states_list.append(next_state); dones_list.append(done)\n",
    "\n",
    "        states = np.array(states_list); actions = np.array(actions_list); rewards = np.array(rewards_list)\n",
    "        next_states = np.array(next_states_list); dones = np.array(dones_list)\n",
    "        \n",
    "        next_actions = np.argmax(learner_model.predict(next_states, verbose=0, batch_size=batch_size), axis=1)\n",
    "        next_q = target_model.predict(next_states, verbose=0, batch_size=batch_size)\n",
    "        target_q = np.array([next_q[i][next_actions[i]] for i in range(batch_size)])\n",
    "        targets = rewards + 0.99 * target_q * (1 - dones)\n",
    "        \n",
    "        current_q = learner_model.predict(states, verbose=0, batch_size=batch_size)\n",
    "        for i, action in enumerate(actions): current_q[i][action] = targets[i]\n",
    "            \n",
    "        loss_history = learner_model.fit(states, current_q, epochs=1, verbose=0, batch_size=batch_size)\n",
    "        history['loss'].append(loss_history.history['loss'][0])\n",
    "        train_steps += 1\n",
    "        \n",
    "        if train_steps % 10 == 0: target_model.set_weights(learner_model.get_weights())\n",
    "        if train_steps % 20 == 0: learner_model.save_weights(weights_path)\n",
    "        \n",
    "        print(f\"[Learner] Progress: {episodes_completed}/{total_episodes} episodes. Training steps: {train_steps}\", end='\\r')\n",
    "\n",
    "    print(\"\\n[Learner] Training complete. Stopping actors...\")\n",
    "    stop_event.set()\n",
    "    learner_model.save_weights(\"final_a3c_weights.h5\")\n",
    "    print(\"Final weights saved to 'final_a3c_weights.h5'\")\n",
    "    return history\n",
    "\n",
    "def plot_results(history):\n",
    "    print(\"Generating plots...\")\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 24))\n",
    "    (ax1, ax2, ax3, ax4) = axes\n",
    "    \n",
    "    ax1.plot(history['scores'], label='Score per Episode', color='royalblue')\n",
    "    ax1.set(title='Agent Score Over Time', xlabel='Episode', ylabel='Total Reward')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history['max_speed'], label='Max Speed', color='purple')\n",
    "    ax2.set(title='Max Speed Achieved per Episode', xlabel='Episode', ylabel='Max Speed')\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.plot(history['loss'], label='Training Loss', color='orangered', alpha=0.7)\n",
    "    ax3.set(title='Model Loss Over Time', xlabel='Training Step', ylabel='MSE Loss')\n",
    "    ax3.legend()\n",
    "\n",
    "    episodes_range = range(len(history['checkpoints']))\n",
    "    ax4.bar(episodes_range, history['checkpoints'], color='forestgreen', label='Checkpoints')\n",
    "    ax4.set(title='Checkpoints Cleared per Episode', xlabel='Episode', ylabel='Checkpoints Cleared')\n",
    "    ax4.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da2e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Learner] Started. Initializing...\n",
      "[Learner] Ready. Waiting for data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     actors\u001b[38;5;241m.\u001b[39mappend(actor)\n\u001b[0;32m     20\u001b[0m     actor\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m---> 22\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWEIGHTS_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_EPISODES_TO_TRAIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m actor \u001b[38;5;129;01min\u001b[39;00m actors:\n\u001b[0;32m     25\u001b[0m     actor\u001b[38;5;241m.\u001b[39mjoin()\n",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m, in \u001b[0;36mrun_learner\u001b[1;34m(experience_queue, weights_path, stop_event, total_episodes)\u001b[0m\n\u001b[0;32m     84\u001b[0m         episodes_completed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) \u001b[38;5;241m<\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m; \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     89\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(memory, batch_size)\n\u001b[0;32m     91\u001b[0m states_list, actions_list, rewards_list, next_states_list, dones_list \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Use 1 less than your number of logical cores (e.g., 8 cores -> 7 actors)\n",
    "    NUM_ACTORS = mp.cpu_count() - 1 \n",
    "    TOTAL_EPISODES_TO_TRAIN = 200\n",
    "    WEIGHTS_FILE = \"a3c_shared_weights.h5\"\n",
    "\n",
    "    # --- Setup and Run ---\n",
    "    # The 'if __name__ == \"__main__\"' is crucial for multiprocessing to work correctly in scripts/notebooks\n",
    "    with mp.Manager() as manager:\n",
    "        experience_queue = manager.Queue()\n",
    "        stop_event = manager.Event()\n",
    "        actors = []\n",
    "        for i in range(NUM_ACTORS):\n",
    "            actor = mp.Process(target=run_actor, args=(i, experience_queue, WEIGHTS_FILE, stop_event))\n",
    "            actors.append(actor)\n",
    "            actor.start()\n",
    "\n",
    "        training_history = run_learner(experience_queue, WEIGHTS_FILE, stop_event, TOTAL_EPISODES_TO_TRAIN)\n",
    "        \n",
    "        for actor in actors:\n",
    "            actor.join()\n",
    "\n",
    "    if training_history:\n",
    "        plot_results(training_history)\n",
    "\n",
    "    print(\"All processes finished. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41205ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIAGNOSTIC CELL ---\n",
    "def simple_actor_test():\n",
    "    print(\"--- Running Diagnostic Test ---\")\n",
    "    # Setup a headless pygame environment\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "    pygame.init()\n",
    "    \n",
    "    # Load the track\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "    track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "    \n",
    "    \n",
    "    # Spawn a car at the start position\n",
    "    car = Car(CAR_IMAGE_PATH, DEFAULT_START_X, DEFAULT_START_Y)\n",
    "    \n",
    "    # Get the color of the pixel directly under the car\n",
    "    pixel_color = track_surface.get_at((int(car.x), int(car.y)))[:3]\n",
    "    \n",
    "    print(f\"Car spawned at: ({int(car.x)}, {int(car.y)})\")\n",
    "    print(f\"Color under the car: {pixel_color}\")\n",
    "    \n",
    "    # Check this color against your constants\n",
    "    print(f\"Your Road Color (DRAW_COLOR) is: {DRAW_COLOR}\")\n",
    "    print(f\"Your Wall Color (BG_COLOR) is: {BG_COLOR}\")\n",
    "    \n",
    "    if pixel_color == BG_COLOR:\n",
    "        print(\"\\nDIAGNOSIS CONFIRMED: The car spawns on the road color.\")\n",
    "        print(\"Your crash logic incorrectly treats this as a crash, causing the actors to fail.\")\n",
    "    else:\n",
    "        print(\"\\nDIAGNOSIS FAILED: Something else is wrong.\")\n",
    "\n",
    "# Run the test\n",
    "simple_actor_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cac7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tf (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
