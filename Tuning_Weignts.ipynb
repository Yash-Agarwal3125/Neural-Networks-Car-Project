{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89326baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Core_Game_Parts import *\n",
    "import os\n",
    "import pygame\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4e6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rl():\n",
    "    \"\"\"Builds the model with a linear output layer suitable for Q-learning.\"\"\"\n",
    "    net = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='linear') # Linear output for Q-values\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    net.compile(optimizer=optimizer, loss='mse')\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836fd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    A Deep Q-Network (DQN) agent for reinforcement learning.\n",
    "    Attributes:\n",
    "        model: The neural network model used for approximating Q-values.\n",
    "        memory: A deque to store past experiences for experience replay.\n",
    "        gamma: Discount factor for future rewards.\n",
    "        epsilon: Exploration rate for the epsilon-greedy policy.\n",
    "        epsilon_min: Minimum exploration rate.\n",
    "        epsilon_decay: Decay rate for exploration after each training episode.\n",
    "        batch_size: Size of the minibatch for training.\n",
    "    Methods:\n",
    "        remember: Store an experience in memory.\n",
    "        choose_action: Select an action based on the current state using an epsilon-greedy policy.\n",
    "        train_from_memory: Train the model using a minibatch of experiences from memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_fn):\n",
    "        self.model = model_fn()\n",
    "        self.target_model = model_fn()\n",
    "        self.update_target_model()\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0 # Will be reset after loading weights\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.batch_size = 128\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(3)\n",
    "        q_values = self.model.predict(np.reshape(state, [1, 4]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train_from_memory(self):\n",
    "        if len(self.memory) < self.batch_size: return None\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([exp[0] for exp in minibatch])\n",
    "        actions = np.array([exp[1] for exp in minibatch])\n",
    "        rewards = np.array([exp[2] for exp in minibatch])\n",
    "        next_states = np.array([exp[3] for exp in minibatch])\n",
    "        dones = np.array([exp[4] for exp in minibatch])\n",
    "        \n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        targets = rewards + self.gamma * np.amax(next_q_values, axis=1) * (1 - dones)\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            current_q_values[i][action] = targets[i]\n",
    "            \n",
    "        history = self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return history.history['loss'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f60ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_game_step(action, car, track_image, current_checkpoint):\n",
    "    done = False\n",
    "    reward = -0.05\n",
    "    if car.speed < 0.2: reward -= 0.5\n",
    "    reward += car.speed * 0.2\n",
    "    current_state, _ = ray_casting(car, track_image)\n",
    "    reward += current_state[1] * 0.01\n",
    "\n",
    "    car.speed += ACCELERATION\n",
    "    if car.speed > 0:\n",
    "        speed_factor = car.speed / MAX_SPEED\n",
    "        turn_angle = MAX_TURN_ANGLE - (speed_factor) * (MAX_TURN_ANGLE - MIN_TURN_ANGLE)\n",
    "        if action == 0: car.angle += turn_angle\n",
    "        elif action == 1: car.angle -= turn_angle\n",
    "    if action == 2: car.speed -= BRAKE_FORCE\n",
    "    car.speed -= FRICTION\n",
    "    car.speed = max(0, min(car.speed, MAX_SPEED))\n",
    "    car.move()\n",
    "    \n",
    "    checkpoint_rects = [pygame.Rect(x,y,w,h) for x,y,w,h,a in checkpoint_data]\n",
    "    if current_checkpoint < len(checkpoint_rects):\n",
    "        if car.rect.colliderect(checkpoint_rects[current_checkpoint]):\n",
    "            current_checkpoint += 1\n",
    "            reward += 100\n",
    "            print(f\"Checkpoint {current_checkpoint} reached!\")\n",
    "    try:\n",
    "        if track_image.get_at((int(car.x), int(car.y)))[:3] == DRAW_COLOR: done = True  #chanhe to track_surface if not working \n",
    "    except IndexError: done = True\n",
    "    if done: reward = -100\n",
    "    new_state, _ = ray_casting(car, track_image)\n",
    "    return new_state, done, reward, current_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb36d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_finetune(episodes=500, pretrained_weights_path=\"pretrained_weights.h5\"):\n",
    "    pygame.init()\n",
    "    track_surface = pygame.image.load(TRACK_IMAGE_PATH).convert()\n",
    "    agent = DQNAgent(model_rl)\n",
    "\n",
    "    try:\n",
    "        agent.model.load_weights(pretrained_weights_path)\n",
    "        agent.update_target_model()\n",
    "        print(f\"✅ Successfully loaded pre-trained weights from {pretrained_weights_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load pre-trained weights: {e}. Starting from scratch.\")\n",
    "\n",
    "    agent.epsilon = 0.1 # Start with low epsilon for fine-tuning\n",
    "    print(f\"Starting fine-tuning with epsilon = {agent.epsilon}\")\n",
    "    \n",
    "    history_data = {'scores': [], 'loss': [], 'checkpoints': [], 'max_speed': []}\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        car = Car(CAR_IMAGE_PATH, DEFAULT_START_X, DEFAULT_START_Y, DEFAULT_START_ANGLE)\n",
    "        current_checkpoint, total_reward, max_speed_episode = 0, 0, 0\n",
    "        distances, _ = ray_casting(car, track_surface)\n",
    "        state = np.array(distances + [car.speed / MAX_SPEED])\n",
    "        \n",
    "        for step in range(5000): # Max steps per episode\n",
    "            action = agent.choose_action(state)\n",
    "            distances_next, done, reward, new_checkpoint = model_game_step(action, car, track_surface, current_checkpoint)\n",
    "            \n",
    "            max_speed_episode = max(max_speed_episode, car.speed)\n",
    "            total_reward += reward\n",
    "            next_state = np.array(distances_next + [car.speed / MAX_SPEED])\n",
    "            current_checkpoint = new_checkpoint\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            if step % 8 == 0:\n",
    "                loss = agent.train_from_memory()\n",
    "                if loss is not None: history_data['loss'].append(loss)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if (e + 1) % 5 == 0:\n",
    "            agent.update_target_model()\n",
    "            print(f\"--- Target Network Updated at Episode {e+1} ---\")\n",
    "            \n",
    "        history_data['scores'].append(total_reward)\n",
    "        history_data['checkpoints'].append(current_checkpoint)\n",
    "        history_data['max_speed'].append(max_speed_episode)\n",
    "        \n",
    "        print(f\"E{e+1}/{episodes}, Score:{total_reward:.2f}, Speed:{max_speed_episode:.2f}, Ckpt:{current_checkpoint}, Eps:{agent.epsilon:.2f}\")\n",
    "        \n",
    "        if (e + 1) % 50 == 0:\n",
    "            # Save with a new name to distinguish from pre-trained weights\n",
    "            agent.model.save_weights(f\"finetuned_weights_episode_{e+1}.h5\")\n",
    "            print(f\"Saved fine-tuned weights at episode {e+1}\")\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "    # --- Plotting the results ---\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 22))\n",
    "    (ax1, ax2, ax3, ax4) = axes\n",
    "    \n",
    "    ax1.plot(history_data['scores'], label='Score per Episode', color='royalblue')\n",
    "    ax1.set(title='Agent Score Over Time', xlabel='Episode', ylabel='Total Reward')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history_data['max_speed'], label='Max Speed', color='purple')\n",
    "    ax2.set(title='Max Speed Achieved per Episode', xlabel='Episode', ylabel='Max Speed')\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.plot(history_data['loss'], label='Training Loss', color='orangered', alpha=0.7)\n",
    "    ax3.set(title='Model Loss Over Time', xlabel='Training Step', ylabel='MSE Loss')\n",
    "    ax3.legend()\n",
    "\n",
    "    episodes_range = range(len(history_data['checkpoints']))\n",
    "    ax4.bar(episodes_range, history_data['checkpoints'], color='forestgreen', label='Checkpoints')\n",
    "    ax4.set(title='Checkpoints Cleared per Episode', xlabel='Episode', ylabel='Checkpoints Cleared')\n",
    "    ax4.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca5e2ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dqn_finetune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain_dqn_finetune\u001b[49m(episodes=\u001b[32m1000\u001b[39m, pretrained_weights_path=\u001b[33m\"\u001b[39m\u001b[33mpretrained_weights.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dqn_finetune' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    train_dqn_finetune(episodes=1000, pretrained_weights_path=\"pretrained_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6d48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
